#type:     node
#context:  top
#internal: labs::ml_cv_rop_synthetic_data::1.0
#icon:     /labs_icons/sidefxlabs_default.svg
#since:    20.5

= Labs ML CV ROP Synthetic Data =

"""The base template for synthetic dataset pipelines."""


The synthetic data pipeline generates data adhering to the [COCO (Common Objects in Context)|https://cocodataset.org/#home] format, a widely used standard for computer vision model training. This format includes several types of ground truth annotations: bounding boxes (bbox), segmentations (RLE and polygon), and keypoints. Its extensibility also allows us to enhance the base schema with additional signals made possible by synthetic data.

This PDG node serves as base template for dataset pipelines. It manages a dataset from content generation to annotation export. It sets up global PDG attributes that can be used for seeding content generation at various stages of a pipeline. It is designed to work with other Synthetics nodes like the [Labs ML CV ROP Annotation Output SOP|Node:sop/labs--ml_cv_rop_annotation_output-1.0] and the [Labs ML CV Synthetics Karma ROP|Node:lop/labs--ml_cv_synthetics_karma_rop-1.0].

TIP:
    It will take longer to cook this node the first you run it in a new directory, because it needs to set up the Python virtual environment. The Python virtual environment will be saved under `$HIP/ml/labs/` by default. You can speed up the process next time by copying the existing `/ml/labs/` folder and its contents to a new `$HIP` directory.

NOTE:

    The default attributes are:

    `@variant`: The number of variants generated by the 'Variant Count' parameter.

    `@frame_index`: The frame number specified in the 'Frame Range' parameter, this repeats per variant.

    `@seed`: A random float generated for each frame; this is unique per frame/variant.

    `@v_seed`: A random float generated per variant but is consistent per frame.

    `@frame_name`: The string used for file output naming. 

    `@ds_dir`: The path to the dataset render directory. 

    `@ds_major`: The major version number of the dataset. 

    `@ds_minor`: The minor version number of the dataset. 

    `@res`: The resolution of the images in the dataset.

:box:
    #display: raised yellow

    The original ML Computer Vision tools were developed by the Synthetic Data team at Endava PLC.


@parameters

    == Cook Controls ==

    Generate Static Work Items:
        #id: generatestatic
        Generates static work items in all nodes in the TOP network. None of the work items will be cooked, and dynamic nodes will do nothing.
        
    Cook Output Node:
        #id: cookbutton
        Cook the output node in the TOP network and any out-of-date nodes connected to it. If you want to recook all the nodes in the TOP network, click Dirty All first and then click this button.
        
    Dirty All:
        #id: dirtyall
        Dirties all the work items in all of the TOP nodes inside the network and marks every node inside the network as needing to recook.
        
    Cancel Cook:
        #id: cancelbutton
        If any nodes in the TOP network are currently cooking, cancels their cook.
        
    Delete All File Outputs From Disk:
        #id: deletefileoutputs
        Deletes all previous cook files from disk and then dirties all the work items in all of the TOP nodes inside the network.

    Custom Python Bin:
        #id: python
        The path to a custom Python 3.11 executable. (If you are on macOS or Linux, please leave this field blank). This is temporarily necessary for Windows users due to an OpenEXR compatibility issue with the current Houdini version. Please download a Python 3.11 executable of your choice. For example, you can visit the official [Python Release Python 3.11.0|https://www.python.org/downloads/release/python-3110/], scroll to the Files section, and download Windows installer (64-bit). Unless you manually changed it during installation, the default executable path may be: `C:\Users\<username>\AppData\Local\Programs\Python\Python311\python.exe`.
        
    == Dataset Controls ==

    Render Directory:
        #id: renderdir
        The location where the dataset will be rendered.
        
    Delivery Directory:
        #id: deliverydir
        Specifies the processed dataset will be saved, this version will only contain the files essential for training.
        
    Debug Dataset and Skip Delivery:
        #id: debugds
        Specifies the major and minor version of the dataset.
        Major version: Typically used for major changes to a dataset, will cause dataset seed to change and shuffle per frame seeds. 
        Minor version: Used for incremental changes that maintain the same master variant seed.
        
    Annotations ROP:
        #id: annotationroppath
        Path to the ML CV ROP Annotation Output node in SOP context.
        
    Synth Render ROP:
        #id: renderroppath
        Path to the ML CV Synthetics Karma ROP LOP configured for the RGBA pass.
        
    Synth GT Render ROP:
        #id: gtrenderroppath
        Path to the ML CV Synthetics Karma ROP LOP configured for the GT pass.
        
    == FiftyOne ==

    View Data Set on Complete:
        #id: viewds
        Opens the current dataset in an instance of Fifty One once it has completed rendering.
        
    View Current Dataset:
        #id: viewdscurrent
        Opens the dataset version defined in the "Dataset Version" parameter in an instance of Fifty One.
        
    View Other Dataset:
        #id: viewdsother
        Opens a file browser to select the root directory of another COCO dataset for viewing. Once selected, the dataset will open in a Fifty One instance.
        
    == Dive Targets ==

    Custom Per Variant:
        #id: customvariant
        An internal subnet for additional top nodes to add variation per variant. Example: changing the light intensity per variant.
        
    Custom Per Frame:
        #id: customframe
        An internal subnet for additional top nodes to add variation per frame.
        
    Image Compositor:
        #id: imagecomp
        An internal copnet for additional control over the look of the image for more variation between images in the dataset.
        
    == Dataset Settings ==

    Skip JSON:
        #id: skipjson
        Skip JSON output, useful for testing and debugging renders.
        
    Dataset Version Major:
        #id: dsversionmajor
        The version of the data set.

        Example: if dataset version major is 3 and dataset version minor is 2 the dataset version is ds3.2
        
    Dataset Version Minor:
        #id: dsversionminor
        The sub-version of the data set.

        Example: if dataset version major is 3 and dataset version minor is 2 the dataset version is ds3.2
        
    === Variant ===

    Variant Count:
        #id: varcount
        Total number of scene variants to be generated for the dataset.
        Each variant will be associated with a @variant and @v_seed PDG attribute, which can be utilized to control parameters.
        
    Render Variant Range:
        #id: rendervarrange
        Enables the use of variant range.
        
    Variant Range:
        #id: varrange
        Specifies the range of variants to render, useful when distributing renders across multiple computers.
        
    === Info ===

    Description:
        #id: description
        Optional input to provide a brief overview of the dataset, including its purpose, content, and any key features or highlights.
        Written out to the “info” array of the coco.json file
        
    Contributor:
        #id: contributor
        Optional input for including the names, roles, affiliations, and contact information of individuals or organizations involved in creating the dataset.
        Written out to the “info” array of the coco.json file.
        
    Notes:
        #id: notes
        Optional input for dataset changelog information. Written out to the “info” array of the coco.json file.
        
    == Render Settings ==

    Skip Render:
        #id: skiprender
        Toggle to skip Karma rendering, allowing re-cooking of the ROP Annotation output without re-rendering the dataset.
        
    Start/End/Inc:
        #id: f
        Start and end frames of the sequence to be rendered for each variant.
        For non-temporal datasets, maintain the values at 1 for both the start and end frames.
        
    Render Partial Sequence:
        #id: renderseqperc
        Enables rendering of a specified percentage of a larger dataset. Useful for previewing a sample spread of variants to ensure desired distributions are achieved.
        
    Percentage to Render:
        #id: renderperc
        Enables rendering of a specified percentage of a larger dataset.
        Useful for previewing a sample spread of variants to ensure desired distributions are achieved. 

        Note: Value is 0-100% This is not an exact percentage, 1% of a 100-frame dataset may result in 0 or 2 frames.
        
    Resolution:
        #id: res
        Resolution of the dataset’s images.
        
    == Visualize ==

    Visualize Distributions:
        #id: visdistro
        Plots the synth attribute with matplotlib and visualize it in a new window to make sure the dataset distribution is good.
        
    Filter by Category:
        #id: filtercat
        Filters by category id from the COCO JSON.
        
    Category Name:
        #id: catname
        String name of the category that will be associated with the integer category ID.

    Synth Attributes:
        #id: synthattr
        The number of synthetic attributes.
        
    === Synth Attributes ===

    Synth Attribute:
        #id: synthattr#
        Name of the synth attribute to be exported to COCO JSON.
        
    == Validation Settings ==

    Skip Validation:
        #id: validation
        Skips dataset validation.
        
    Validate Keypoints:
        #id: checkkp
        Enables keypoint validation ensuring all keypoints are correctly in the dataset.

    Categories:
        #id: category
        The number of categories.
        
    === Annotation Property ===

    Annotation:
        #id: anotation_name#_#
        COCO Annotation name to validate.
        
    == Image Compositing ==

    Use Shadow Matte:
        #id: useshadows
        Enables the use of a Shadow Matte set up in the `Synthetics Karma ROP` resulting in a shadow being composited to ground the object to its backplate.
        
    Composite Background:
        #id: usebg
        Enables compositing each foreground image onto a random background image from a specified directory of backgrounds.
        
    Background Images:
        #id: bgimages
        If enabled takes a file pattern with filters for the backgrounds to randomly be composited. 
        
    === Post Comp ===

    Enable Clipping:
        #id: useclipping
        Enables clipping tab.
        
    Enable Grain:
        #id: usegrain
        Enables grain controls tab.
        
    Fill Alpha:
        #id: fillalpha
        Enables a process that ensures the output frames have no transparent pixels.

    === Grade ===

    Brightness Min/Max:
        #id: brightminmax_
        Defines the allowable range for the brightness factor to be randomly applied per image.

    Brightness:
        #id: brightamount
        The scale factor to control how bright or dim the layer is. Higher values increase the brightness. Lower values decrease the brightness. Defaults to an expression controlled by the brightness min/max.
        
    === Levels ===

    Input Levels:
        #id: inputlevels_
        Adjusts black and white points to increase contrast.
        
    Gamma:
        #id: gamma
        Adjusts midrange balance.
        
    Output Levels:
        #id: output_levels_
        Remaps the result of Input Levels and Gamma to reduce contrast.
        
    === Blur ===

    Filter:
        #id: filter
        Selects between box blur and Gaussian blur.
            
    Read Pixels outside Image:
        #id: readoutsidepixels
        Defines edge of frame behavior.
        
    Units:
        #id: units
        Defines the units that the diameter of the blur is expressed in.
            
    Blur Min/Max:
        #id: blurminmax_
        Defines the allowable range for the amount of blur to be randomized per image.
        
    Size:
        #id: size
        Defines the diameter of the blur. Defaults to an expression controlled by the blur min/max.
        
    Scale Size:
        #id: scalesize_
        Disproportionately scales the blur on either axis.
        
    === Clipping ===

    Mask:
        #id: mask
        Blends the modified image with the unclipped image.
        
    Lower Limit:
        #id: clipping_lowerlimit
        Clips the black point in the render.
        
    Upper Limit:
        #id: clipping_upperlimit
        Clips the white point in the render.
        
    Clamped Values:
        #id: clippingmethod
        Specifies how clamped values should be handled.
            
    === Grain ===

    Grain Min/Max:
        #id: grainminmax_
        Specifies the allowable range of grain amplitude to be randomized per image.
        
    Grain Amplitude:
        #id: grainamp
        Specifies the intensity of the grain. Defaults to an expression controlled by the __Grain Min/Max__.
        
    Element Size:
        #id: elementsize
        The size, in image coordinates, of the basic element of the grain.
        
    Contrast:
        #id: contrast
        Used to make the noise appear more extreme without exceeding the 0 to 1 range.
        
    Seed:
        #id: grainseed
        Randomizes the grain pattern.
        
    == Advanced ==
        
    Base Frame Format:
        #id: basename
        The base filename used to generate data, by default it is derived from the frame number and variant number.
        Changing this is likely to cause issues with the automated portions of the PDG graph.
        
    Dataset Seed:
        #id: dsseed
        Seed that controls variation in a dataset, this is driven by the dataset major version.

    Operating System:
        #id: os
        The current operating system.
        
    === Virtaul Environment ===

    Environment Path:
        #id: venvpath
        The path to the python virtual environment in which the internal training script of this node is run.
          

@examples

    TIP:
        When viewing in Houdini's Help Browser, please copy the example file's URL to a regular browser to proceed with the download.

    - [Example File|https://github.com/sideeffects/SideFXLabsExamples/blob/main/examples/ml_computer_vision/ml_computer_vision.1.0.zip]


@related